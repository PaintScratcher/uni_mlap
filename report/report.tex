\documentclass{article}
\RequirePackage[numbers,sort&compress,square,comma]{natbib}
\bibliographystyle{IEEEtranN}
\begin{document}
\title{Machine Learning and Applications (MLAP) Open Assessment}
\author{Exam Number: Y0070813}
\maketitle
%\tableofcontents
\section{Task 1}
\section{Task 2}
\section{Task 3}
\subsection{1. Qualitative Description of the algorithms}
\subsubsection{Locally Linear Embedding}
LLE first generates a list of neighbours for each data-point on the manifold. Then, using the assumption that the neighbourhood lies on a locally linear plane, a barycentric coordinate for each data-point as a series of weights for each of its neighbours such that the data-point can be re-constructed as a linear combination of its neighbours. These weights are then used in the lower dimensional space to re-construct the data-point. As each locally linear neighbourhood is overlapping, it is possible to re-construct the entire dataset in the low dimensional space. An error function is used for both the weight generation and point re-construction phases, to produce an optimal solution.
\subsubsection{Laplacian eigenmaps}
In this algorithm, we first generate a neighbourhood graph. Then a weight is applied to each edge, using one of two methods, either simply applying a weight of 1 for each edge (simple-minded) or by using a heat kernel. A Laplacian matrix is then generated from the weight matrix and its diagonal. By solving the generalized eigenvector problem for this Laplacian matrix and the weight matrix you find several solutions. These solutions are then ordered my ascending sizes of the eigenvalues. After removing the first eigenvalue, which will be zero, each of these solutions can be used as an embedding dimension for each data-point.
\subsubsection{Isomap}
Isomap can be thought of as an extension of Multi Dimensional Scaling, as it attempts to plot every data point in a lower dimensional space such that the high dimensional distances between data-points are maintained as best as possible. The distance metric is an estimate of geodesic distance between points over the manifold, generated by summing the euclidean distance between nodes on the shortest path between the nodes over a neighbourhood graph. Step 1 generates the neighbourhood graph, either with k-nearest neighbours or fixed radius, then the shortest paths between nodes are calculated and finally the estimated geodesic distance is used to apply MDS.
\subsection{2. Comparing the methods for embedding data into vector space}
ISOMAP PROS
preserves global structure
few free parameters
Guarantee of globally optimality
ISOMAP CONS
difficulties with holes in manifold
Sensitive to noise - short circuit 
Computationally intense due to dense matrix eigen reduction 
LLE PROS
No Local minima
one free parameter
simply linear algebra operations - so quick - eigenvectors from sparse matrices 
LLE CONS
can distort global structure 
***Seems to crush clusters into a single point?
Requires dense datapoints
LE PROS
Intrinsically emphasizes natural clusters 
stability due to intrinsic consideration of manifold structure
LE CONS
Has problems with non-uniform sampling?

local vs global methods
- Global gives more faithful representation of the global structure
- Local is more computationally efficient due to sparse matrix computations 
- Local may give useful results on a broader range of manifolds
http://papers.nips.cc/paper/2141-global-versus-local-methods-in-nonlinear-dimensionality-reduction.pdf
\subsection{3. Describing the methods mathematically}
\subsubsection{Locally Linear Embedding}
LLE begins by computing the neighbours for each data-point $x_i$ in $X = \{\vec{x}_1,...,\vec{x}_N\}$ either by placing a threshold on euclidean distance $\epsilon$, such that $x_j$ is a neighbour of $x_i$ if the euclidean distance in the high dimensional space $d_x(x_i,x_j) < \epsilon$. Alternatively, neighbours can be assigned based on $k$ nearest neighbours. We now need to compute the weights for each of the neighbours such that we can reconstruct the data-point as a linear combination of its neighbours. The following method is descried here.\cite{LLERoweis} First, we take the matrix $Z$, consisting of every neighbour of $x_i$ and subtract $x_i$ form every neighbour, we then compute the local co-variance of each element: $C = Z^TZ$. By solving $Cw = 1$ we find the weightings for each neighbour. We create an N by N matrix $W$ to store these weights, with $W_{ij}$ being 0 if $x_j$ is not a neighbour of $x_i$ and $\frac{w}{sum(w)}$ if they are neighbours.\\
We can now use this weight matrix to reconstruct $X$ in a lower dimensional space $d<<D$ with embedding $Y$. We do this by minimising the error function $\min_Y\sum\limits_{i}|Y_i-\sum\limits_{j}W_{ij}Y_j|^2$.\cite{ghodsi2006dimensionality} We can solve this error function by finding the eigenpairs for the matrix $M = (I-W)^T(I-W)$ and setting the rows of $Y$ to the ascending eigenvalues.
\subsubsection{Laplacian Eigenmaps}

\subsubsection{Isomap}
Starting with a set of $N$ data-points $X = \{\vec{x}_1,...,\vec{x}_N\}$ we first create a weighted graph, $G$, of the neighbourhood relations of X. For each data-point $x_i$ we calculate its neighbouring points $X_n = \{\vec{x}_1,...,\vec{x}_j\}$ either by placing a threshold on euclidean distance $\epsilon$, such that $x_j$ is a neighbour of $x_i$ if the euclidean distance in the high dimensional space $d_x(x_i,x_j) < \epsilon$. Alternatively, neighbours can be assigned based on $k$ nearest neighbours. An edge is then placed in $G$ between each data-point and its neighbours, with the edge weight being the euclidean distance between them. \\
Now we must generate a distance matrix $D_G$ of the geodesic distance between all points in $G$. We initialise this matrix by inserting the euclidean distance between points that are connected, $d_G(i,j) = d_x(i,j)$ if $i$ and $j$ are connected in the graph. If the points are not connected we need to estimate the geodesic distance between them by minimising the combined weights of the edges in the path between them over the graph. This is effectively finding the shortest path between every two points. In Isomap, this is achieved using the Floydâ€“Warshall algorithm.\cite{floyd1962algorithm} Thus $ d_G(i,j) = min\{d_G(i,j),d_G(i,k) + d_G(k,j)\}$ for all values of k. \\
Now that we have a pairwise geodesic distance matrix for $X$ we can apply the classic Multi Dimensional Scaling algorithm by calculating a kernel matrix, by double centring the distance matrix: $K = -\frac{1}{2}(I - \frac{J}{n})D_G(I-\frac{J}{n}))$ where J is an n-by-n matrix of 1's and I is the identity matrix. We then eigendecompose the kernel matrix $K = U\Lambda U^T$. After removing negative eigenvalues we then set $Y = \Lambda^{\frac{1}{2}}U^T$. The dimensionality of $Y$ is adjustable by removing eigenvalues and vectors from their respective matrices, in order of size.

All algorithms and formulas taken from \textit{'A Global Geometric Framework for Nonlinear Dimensionality Reduction'}\cite{tenenbaum2000global} except where cited.

\subsection{4. Description of a paper}
\bibliography{report}{}
\bibliographystyle{IEEEtranN}
\end{document}